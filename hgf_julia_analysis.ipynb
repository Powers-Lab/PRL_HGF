{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43pPolH39V0G"
      },
      "source": [
        "# HGF Modeling of PRL Data for Max's Psychedelics Disseration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhC8aC-GEEia"
      },
      "source": [
        "# Setup \n",
        "\n",
        "## Get correct packages and dependencies and read in Peter's HGF script\n",
        "Make sure you have the Project and Manifest.toml in your current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Initialize Environment and instantiate based on .toml\n",
        "import Pkg\n",
        "cd(\"C:/Users/maxsu/DataAnalysis/julia_hgf\") #cd to directory with the correct project.toml and manifest.toml\n",
        "Pkg.activate(\".\") #activate environment in current directory (assuming we're in above directory)\n",
        "Pkg.instantiate() #ensures all correct packages and dependencies in .tomls are installed\n",
        "# include(\"create_hgf.jl\")  # Use all of the functions and whatnot that Peter created\n",
        "\n",
        "### Load libraries\n",
        "using HierarchicalGaussianFiltering\n",
        "using ActionModels\n",
        "using StatsPlots\n",
        "using Distributions\n",
        "using Turing\n",
        "using CSV\n",
        "using DataFrames\n",
        "using Plots\n",
        "using Logging\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_GfaV4bEEib",
        "outputId": "2bc602d6-6f63-48fd-c5f7-eb9c0749d2a8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### READ IN AND REFORMAT DATA ###\n",
        "df = DataFrame(CSV.File(\"./Data/prl_julia.csv\"))\n",
        "\n",
        "#Get list of participant ids\n",
        "unique_values = unique(df.record_id)\n",
        "println(\"Unique record_id values: \", unique_values)\n",
        "\n",
        "participant = 60\n",
        "\n",
        "#select the inputs for other participants\n",
        "df_inputs_participant = df[df.record_id .== participant, [\"fractal1HGF\", \"fractal2HGF\", \"fractal3HGF\"]] #MUST BE IN THIS ORDER FOR XPROB TO LINE UP WITH BANDITS!\n",
        "\n",
        "#select the observations for the action model\n",
        "df_obs_participant = df[df.record_id .== participant, [\"fractalselected\", \"outcome\"]]\n",
        "\n",
        "#Also get whole dataset for just one participant\n",
        "df_participant = df[df.record_id .== participant,:]\n",
        "\n",
        "#select the reward probabilities for simulated_actions\n",
        "df_probs_participant = df[df.record_id .== participant, [\"rewardProbFractal1\", \"rewardProbFractal2\", \"rewardProbFractal3\"]]\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Create HGF and Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-- Agent struct --\n",
            "Action model name: choose_bandit\n",
            "Substruct type: HierarchicalGaussianFiltering.HGF\n",
            "Number of parameters: 19\n",
            "Number of states (including the action): 47\n"
          ]
        }
      ],
      "source": [
        "######## CREATE HGF ######\n",
        "config = Dict()\n",
        "\n",
        "#Defaults\n",
        "spec_defaults = Dict(\n",
        "    \"n_bandits\" => 3,\n",
        "\n",
        "    (\"xprob\", \"volatility\") => -2,\n",
        "    (\"xprob\", \"drift\") => 0,\n",
        "    (\"xprob\", \"autoconnection_strength\") => 1,\n",
        "    (\"xprob\", \"initial_mean\") => 0,\n",
        "    (\"xprob\", \"initial_precision\") => 1,\n",
        "\n",
        "    (\"xvol\", \"volatility\") => -2,\n",
        "    (\"xvol\", \"drift\") => 0,\n",
        "    (\"xvol\", \"autoconnection_strength\") => 1,\n",
        "    (\"xvol\", \"initial_mean\") => 0,\n",
        "    (\"xvol\", \"initial_precision\") => 1,\n",
        "\n",
        "    (\"xbin\", \"xprob\", \"coupling_strength\") => 1,\n",
        "    (\"xprob\", \"xvol\", \"coupling_strength\") => 1,\n",
        "\n",
        "    \"update_type\" => EnhancedUpdate(),\n",
        "    \"save_history\" => true,\n",
        ")\n",
        "\n",
        "#Merge to overwrite defaults\n",
        "config = merge(spec_defaults, config)\n",
        "\n",
        "#Initialize list of nodes\n",
        "nodes = HierarchicalGaussianFiltering.AbstractNodeInfo[]\n",
        "edges = Dict{Tuple{String, String}, HierarchicalGaussianFiltering.CouplingType}()\n",
        "grouped_xprob_volatility = []\n",
        "grouped_xprob_drift = []\n",
        "grouped_xprob_autoconnection_strength = []\n",
        "grouped_xprob_initial_mean = []\n",
        "grouped_xprob_initial_precision = []\n",
        "grouped_xbin_xprob_coupling_strength = []\n",
        "grouped_xprob_xvol_coupling_strength = []\n",
        "\n",
        "#For each bandit\n",
        "for i = 1:config[\"n_bandits\"]\n",
        "\n",
        "    #Add input node\n",
        "    push!(nodes, BinaryInput(\"u$i\"))\n",
        "\n",
        "    #Add binary node\n",
        "    push!(nodes, BinaryState(\"xbin$i\"))\n",
        "\n",
        "    #Add probability node\n",
        "    push!(\n",
        "        nodes,\n",
        "        ContinuousState(\n",
        "            name = \"xprob$i\",\n",
        "            volatility = config[(\"xprob\", \"volatility\")],\n",
        "            drift = config[(\"xprob\", \"drift\")],\n",
        "            autoconnection_strength = config[(\"xprob\", \"autoconnection_strength\")],\n",
        "            initial_mean = config[(\"xprob\", \"initial_mean\")],\n",
        "            initial_precision = config[(\"xprob\", \"initial_precision\")],\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    #Group the parameters for each binary HGF\n",
        "    push!(grouped_xprob_volatility, (\"xprob$i\", \"volatility\"))\n",
        "    push!(grouped_xprob_drift, (\"xprob$i\", \"drift\"))\n",
        "    push!(grouped_xprob_autoconnection_strength, (\"xprob$i\", \"autoconnection_strength\"))\n",
        "    push!(grouped_xprob_initial_mean, (\"xprob$i\", \"initial_mean\"))\n",
        "    push!(grouped_xprob_initial_precision, (\"xprob$i\", \"initial_precision\"))\n",
        "    push!(grouped_xbin_xprob_coupling_strength, (\"xbin$i\", \"xprob$i\", \"coupling_strength\"))\n",
        "    push!(grouped_xprob_xvol_coupling_strength, (\"xprob$i\", \"xvol\", \"coupling_strength\"))\n",
        "\n",
        "    #Add edges\n",
        "    push!(edges, (\"u$i\", \"xbin$i\") => ObservationCoupling())\n",
        "    push!(\n",
        "        edges,\n",
        "        (\"xbin$i\", \"xprob$i\") =>\n",
        "            ProbabilityCoupling(config[(\"xbin\", \"xprob\", \"coupling_strength\")]),\n",
        "    )\n",
        "    push!(\n",
        "        edges,\n",
        "        (\"xprob$i\", \"xvol\") =>\n",
        "            VolatilityCoupling(config[(\"xprob\", \"xvol\", \"coupling_strength\")]),\n",
        "    )\n",
        "\n",
        "end\n",
        "\n",
        "#Add the shared volatility parent\n",
        "push!(\n",
        "    nodes,\n",
        "    ContinuousState(\n",
        "        name = \"xvol\",\n",
        "        volatility = config[(\"xvol\", \"volatility\")],\n",
        "        drift = config[(\"xvol\", \"drift\")],\n",
        "        autoconnection_strength = config[(\"xvol\", \"autoconnection_strength\")],\n",
        "        initial_mean = config[(\"xvol\", \"initial_mean\")],\n",
        "        initial_precision = config[(\"xvol\", \"initial_precision\")],\n",
        "    ),\n",
        ")\n",
        "\n",
        "parameter_groups = [\n",
        "\n",
        "    ParameterGroup(\"xprob_volatility\",\n",
        "        grouped_xprob_volatility,\n",
        "        config[(\"xprob\", \"volatility\")],\n",
        "    ),\n",
        "    ParameterGroup(\"xprob_drift\",\n",
        "        grouped_xprob_drift,\n",
        "        config[(\"xprob\", \"drift\")],\n",
        "    ),\n",
        "    ParameterGroup(\"xprob_autoconnection_strength\",\n",
        "        grouped_xprob_autoconnection_strength,\n",
        "        config[(\"xprob\", \"autoconnection_strength\")],\n",
        "    ),\n",
        "    ParameterGroup(\"xprob_initial_mean\",\n",
        "        grouped_xprob_initial_mean,\n",
        "        config[(\"xprob\", \"initial_mean\")],\n",
        "    ),\n",
        "    ParameterGroup(\"xprob_initial_precision\",\n",
        "        grouped_xprob_initial_precision,\n",
        "        config[(\"xprob\", \"initial_precision\")],\n",
        "    ),\n",
        "    ParameterGroup(\"xbin_xprob_coupling_strength\",\n",
        "        grouped_xbin_xprob_coupling_strength,\n",
        "        config[(\"xbin\", \"xprob\", \"coupling_strength\")],\n",
        "    ),\n",
        "    ParameterGroup(\"xprob_xvol_coupling_strength\",\n",
        "        grouped_xprob_xvol_coupling_strength,\n",
        "        config[(\"xprob\", \"xvol\", \"coupling_strength\")],\n",
        "    ),\n",
        "]\n",
        "\n",
        "#Initialize the HGF\n",
        "hgf = init_hgf(\n",
        "    nodes = nodes,\n",
        "    edges = edges,\n",
        "    parameter_groups = parameter_groups,\n",
        "    verbose = false,\n",
        "    node_defaults = NodeDefaults(update_type = config[\"update_type\"]),\n",
        "    save_history = config[\"save_history\"],\n",
        ")\n",
        "\n",
        "### CREATE AGENT ###\n",
        "\n",
        "#Softmax function\n",
        "function softmax(x::AbstractVector, inv_temp::Real)\n",
        "    exp_values = exp.(x * inv_temp)\n",
        "    prob_values = exp_values / sum(exp_values)\n",
        "    return prob_values\n",
        "end\n",
        "\n",
        "#Action model function\n",
        "function choose_bandit(agent::Agent, input::Any)\n",
        "\n",
        "    ### UPDATE HGF ###\n",
        "    #Unpack the input into which badnit has been observed, and what the observation was\n",
        "    observed_bandit, observation = input\n",
        "\n",
        "    #Extrat the HGF\n",
        "    hgf = agent.substruct\n",
        "\n",
        "    #Create empty vector of observations\n",
        "    hgf_input = Vector{Union{Int, Missing}}(missing, length(hgf.input_nodes))\n",
        "\n",
        "    #Change the missing to the atual observation for the bandit that was observed\n",
        "    hgf_input[observed_bandit] = observation\n",
        "\n",
        "    #Pass the observation to the HGF\n",
        "    update_hgf!(hgf, hgf_input)\n",
        "\n",
        "\n",
        "    ### PICK A BANDIT ###   \n",
        "    #Get the predicted probabilites for reqards for each of the bandits\n",
        "    predicted_probabilities = [hgf.ordered_nodes.input_nodes[i].edges.observation_parents[1].states.prediction_mean for i in 1:length(hgf.input_nodes)]\n",
        "\n",
        "    #Get the precision parameter\n",
        "    β = agent.parameters[\"softmax_precision\"]\n",
        "\n",
        "    #Run them through the softmax\n",
        "    action_proabilities = softmax(predicted_probabilities, β)\n",
        "\n",
        "    #If the probabilities became NaN\n",
        "    if any(isnan, action_proabilities)\n",
        "        #Throw an error that will reject samples when fitted\n",
        "        throw(\n",
        "            RejectParameters(\n",
        "                \"With these parameters and inputs, the probabilities became NaN, which is invalid. Try other parameter settings\",\n",
        "            ),\n",
        "        )\n",
        "    end\n",
        "\n",
        "    #Return a Categorical probability distribution\n",
        "    action_distribution = Categorical(action_proabilities)\n",
        "\n",
        "    return action_distribution\n",
        "end\n",
        "\n",
        "#Add the temeprature parmaeter for the action model\n",
        "parameters = Dict(\"softmax_precision\" => 1)\n",
        "\n",
        "#create the agent\n",
        "agent = init_agent(\n",
        "    choose_bandit, \n",
        "    substruct = hgf,\n",
        "    parameters = parameters\n",
        ")\n",
        "\n",
        "\n",
        "# ### PLAY WITH THE AGENT ###\n",
        "\n",
        "# #See the parameters in the agent\n",
        "# get_parameters(agent)\n",
        "\n",
        "# #Set parameters\n",
        "# set_parameters!(agent, Dict(\"softmax_precision\" => 0.1))\n",
        "# reset!(agent)\n",
        "\n",
        "# #\"real inputs\"\n",
        "# inputs = [\n",
        "#     [1, 1],\n",
        "#     [2, 0],\n",
        "#     [3, 1],\n",
        "#     [1, 0],\n",
        "#     [2, 1],\n",
        "# ]\n",
        "\n",
        "# #Run forward to simulate actions\n",
        "# simulated_actions = give_inputs!(agent,inputs)\n",
        "\n",
        "# #Plot belief trajectories\n",
        "# plot_trajectory(agent, \"xbin1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optional: testing HGF and Response models separately"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "##### TRY OUT HGF #####\n",
        "\n",
        "## READ DATA ##\n",
        "#One vector per timepoint.\n",
        "#Consiting of a vector with a value for each bandit\n",
        "#Which can be 0, 1 or missing (i.e. no observaiton of that bandit)\n",
        "# For example: \n",
        "# inputs = [\n",
        "#     [missing, 1, missing],\n",
        "#     [missing, 0, missing],\n",
        "#     [missing, missing, 1],\n",
        "#     [missing, missing, 1],\n",
        "# ]\n",
        "\n",
        "inputs = [collect(row) for row in eachrow(df_inputs_participant)]\n",
        "\n",
        "#See current parameter values\n",
        "get_parameters(hgf)\n",
        "\n",
        "parameters = Dict(\"xprob_volatility\" => -1.25, #-1.28\n",
        "                    #1.28 seems to be the best number; 1.27-1.28. Cannot be estimated above -1.26, and bad xvol below -1.3\n",
        "                \"xprob_initial_mean\" => 0,\n",
        "                \"xbin_xprob_coupling_strength\" => 1,\n",
        "                    #can't really go over 1\n",
        "                \"xprob_xvol_coupling_strength\" => 1, \n",
        "                    #increasing when xprob_vol low (causing low xvol) leads to flat xprob. \n",
        "                    #When xprob_vol high, higher levels flatten both xprob AND xvol. \n",
        "                    #decreasing to 0.2 flattens xvol and sorta xprob\n",
        "                \"xprob_drift\" => 0,\n",
        "                    # Cannot really change much w/o model freaking out\n",
        "                \"xprob_autoconnection_strength\" => 0.95, \n",
        "                    # HUGE effects from changes -- best range is 0.95 - 1.01.\n",
        "                    # Seems like 1 = not autoconnection, <1 = increasing regression to 0.5, >1 = increasing regression AWAY from 0.5 (lower if last estimate was <0.5; higher if >0.5)\n",
        "                    # Cannot go far above 1!!!\n",
        "                )\n",
        "\n",
        "#Change parameter values\n",
        "set_parameters!(hgf, parameters)\n",
        "reset!(hgf)\n",
        "\n",
        "#Give inputs to the HGF\n",
        "give_inputs!(hgf, inputs)\n",
        "\n",
        "# Plot  belief trajectory for the HGF w/actual reward probabilities overlayed\n",
        "\n",
        "# #but first adjust the reward probabilities to be roughly the min and max for the xprobabilities\n",
        "# df_60_graphing = df_60\n",
        "# cols = names(df_60_graphing)  # Targeting all columns for replacement\n",
        "# for col in cols\n",
        "#     df_60_graphing[!, col] = map(x -> begin\n",
        "#         if ismissing(x)\n",
        "#             x  # Keep missing values as missing\n",
        "#         else\n",
        "#             if x == 0.15 #min prob\n",
        "#                 -4.5\n",
        "#             elseif x == 0.85 #max probability\n",
        "#                 4.5 #max xprob\n",
        "#             elseif x == 1\n",
        "#                 4.3\n",
        "#             elseif x == 0 #if loss, make dot show up at bottom\n",
        "#                 -4.3\n",
        "#             elseif x == 0.5 #50% probabilitiy should be 0 center of graph\n",
        "#                 0.001\n",
        "#             else\n",
        "#                 x  # Return x unchanged if none of the conditions are met\n",
        "#             end\n",
        "#         end\n",
        "#     end, df_60_graphing[!, col])\n",
        "# end\n",
        "\n",
        "#Add colors array for graphing wins vs. losses\n",
        "colors3 = [ismissing(x) ? :black : (x > 0 ? :green : :red) for x in df_participant.fractal3HGF]\n",
        "colors2 = [ismissing(x) ? :black : (x > 0 ? :green : :red) for x in df_participant.fractal2HGF]\n",
        "colors1 = [ismissing(x) ? :black : (x > 0 ? :green : :red) for x in df_participant.fractal1HGF]\n",
        "\n",
        "xvol_plot = plot_trajectory(hgf,\"xvol\")\n",
        "\n",
        "xprob3_plot = plot_trajectory(hgf, \"xbin3\")\n",
        "plot!(xprob3_plot, df_participant.trial, df_participant.rewardProbFractal3, label=\"Bandit 3 true rew prob\", color=:blue, linewidth=3)\n",
        "scatter!(xprob3_plot, df_participant.trial, df_participant.fractal3HGF, label=\"Bandit 3 observations\", color=colors3)\n",
        "\n",
        "xprob2_plot = plot_trajectory(hgf, \"xbin2\")\n",
        "plot!(xprob2_plot, df_participant.trial, df_participant.rewardProbFractal2, label=\"Bandit 2 true rew prob\", color=:blue, linewidth=3)\n",
        "scatter!(xprob2_plot, df_participant.trial, df_participant.fractal2HGF, label=\"Bandit 2 observations\", color=colors2)\n",
        "\n",
        "xprob1_plot = plot_trajectory(hgf, \"xbin1\")\n",
        "plot!(xprob1_plot, df_participant.trial, df_participant.rewardProbFractal1, label=\"Bandit 1 true rew prob\", color=:blue, linewidth=3)\n",
        "scatter!(xprob1_plot, df_participant.trial, df_participant.fractal1HGF, label=\"Bandit 1 observations\", color=colors1)\n",
        "\n",
        "\n",
        "# Combine all plots into a single figure with a specified layout\n",
        "combined_plot = plot(xvol_plot, xprob3_plot, xprob2_plot, xprob1_plot, layout = (4,1), size = (1500,800), legend=false)\n",
        "\n",
        "# Display the combined plot\n",
        "display(combined_plot)\n",
        "\n",
        "# savefig(combined_plot, \"./visualizations/HGFtrajectories_id60.png\")  # Saves the plot as a PNG file\n",
        "\n",
        "\n",
        "\n",
        "# plot_trajectory!(hgf, \"xprob2\")\n",
        "# plot_trajectory!(hgf, \"xvol3\")\n",
        "\n",
        "# plot(\n",
        "#     xprob3_plot=plot_trajectory(hgf, \"xprob3\"), # reward probability for 3rd fractal\n",
        "#     plot!(xprob3_plot, df_60.trial, df_60.rewardProbFractal3, label=\"Bandit 3\", color=:red,linewidth=3)\n",
        "\n",
        "#     xprob2_plot=plot_trajectory(hgf, \"xprob2\"), # reward probability for 2nd  fractal\n",
        "#     plot!(xprob2_plot, df_60.trial, df_60.rewardProbFractal2, label=\"Bandit 2\", color=:blue, linewidth=3)\n",
        "\n",
        "#     xprob1_plot=plot_trajectory(hgf, \"xprob1\"), # reward probability for 1st fractal\n",
        "#     plot!(xprob1_plot,df_60.trial, df_60.rewardProbFractal1, label=\"Bandit 1\", color=:green, linewidth=3)\n",
        "#     layout = (3,1), #3 rows 1 column\n",
        "#     size = (1500,800),\n",
        "#     legend=false,\n",
        "# )\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### TEST AGENT ###\n",
        "\n",
        "#See the parameters in the agent\n",
        "get_parameters(agent)\n",
        "\n",
        "#Set parameters\n",
        "set_parameters!(agent, Dict(\"softmax_temperature\" => 100000000000))\n",
        "reset!(agent)\n",
        "\n",
        "\n",
        "# #\"real inputs\" [banditSelected,outcome]\n",
        "# inputs = [\n",
        "#     [1, 1],\n",
        "#     [2, 0],\n",
        "#     [3, 1],\n",
        "#     [1, 0],\n",
        "#     [2, 1],\n",
        "    \n",
        "# ]\n",
        "\n",
        "inputs = [Int.(collect(row)) for row in eachrow(df_obs_60)] #need to convert float to integer\n",
        "\n",
        "#Run forward to simulate actions\n",
        "simulated_actions = give_inputs!(agent,inputs)\n",
        "\n",
        "#Plot belief trajectories\n",
        "xvol_plot = plot_trajectory(agent,\"xvol\")\n",
        "\n",
        "xprob3_plot = plot_trajectory(agent, \"xbin3\")\n",
        "plot!(xprob3_plot, df_60.trial, df_60.rewardProbFractal3, label=\"Bandit 3 true rew prob\", color=:blue, linewidth=3)\n",
        "scatter!(xprob3_plot, df_60.trial, df_60.fractal3HGF, label=\"Bandit 3 observations\", color=colors3)\n",
        "\n",
        "xprob2_plot = plot_trajectory(agent, \"xbin2\")\n",
        "plot!(xprob2_plot, df_60.trial, df_60.rewardProbFractal2, label=\"Bandit 2 true rew prob\", color=:blue, linewidth=3)\n",
        "scatter!(xprob2_plot, df_60.trial, df_60.fractal2HGF, label=\"Bandit 2 observations\", color=colors2)\n",
        "\n",
        "xprob1_plot = plot_trajectory(agent, \"xbin1\")\n",
        "plot!(xprob1_plot, df_60.trial, df_60.rewardProbFractal1, label=\"Bandit 1 true rew prob\", color=:blue, linewidth=3)\n",
        "scatter!(xprob1_plot, df_60.trial, df_60.fractal1HGF, label=\"Bandit 1 observations\", color=colors1)\n",
        "\n",
        "\n",
        "# Combine all plots into a single figure with a specified layout\n",
        "combined_plot = plot(xvol_plot, xprob3_plot, xprob2_plot, xprob1_plot, layout = (4,1), size = (1500,800), legend=false)\n",
        "\n",
        "# Display the combined plot\n",
        "display(combined_plot)\n",
        "\n",
        "\n",
        "#### SIMULARTING TO SEE WAHT HAPPENS WITH DIFFERENT PARAMETER SETTINGS ####\n",
        "\n",
        "#Set true probs\n",
        "true_probs = [0.1, 0.1, 0.9]\n",
        "#Set initial choice\n",
        "init_choice = rand(Categorical([1/3, 1/3, 1/3]))\n",
        "initial_obs = rand(Bernoulli(true_probs[init_choice]))\n",
        "next_input = (init_choice,initial_obs)\n",
        "#Set agent parameters\n",
        "set_parameters!(agent, Dict(\"softmax_precision\" => 4,\n",
        "                            \"xprob_volatility\" => -2,))\n",
        "reset!(agent)\n",
        "\n",
        "#Save the inputs\n",
        "inputs = []\n",
        "simulated_actions = []\n",
        "\n",
        "push!(inputs, next_input)\n",
        "#Simulate\n",
        "for i = 1:100\n",
        "    #Update HGF and generate action\n",
        "    action = single_input!(agent, next_input)\n",
        "\n",
        "    #generate new observation based on action\n",
        "    new_observation = rand(Bernoulli(true_probs[action]))\n",
        "\n",
        "    #Save the next input as the action and the observation\n",
        "    next_input = (action, new_observation)\n",
        "\n",
        "    push!(simulated_actions, action)\n",
        "    push!(inputs, next_input)\n",
        "end\n",
        "\n",
        "#remove the last input\n",
        "pop!(inputs)\n",
        "\n",
        "#Plot\n",
        "plot_trajectory(agent, \"action\")\n",
        "plot_trajectory(agent, \"xbin1\")\n",
        "plot_trajectory(agent, \"xbin2\")\n",
        "plot_trajectory(agent, \"action\")\n",
        "\n",
        "# length(inputs)\n",
        "# length(simulated_actions)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Simulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Get list of participant ids\n",
        "unique_values = unique(df.record_id)\n",
        "\n",
        "\n",
        "participant = 84\n",
        "\n",
        "#select the inputs for other participants\n",
        "df_inputs_participant = df[df.record_id .== participant, [\"fractal1HGF\", \"fractal2HGF\", \"fractal3HGF\"]] #MUST BE IN THIS ORDER FOR XPROB TO LINE UP WITH BANDITS!\n",
        "\n",
        "#select the observations for the action model\n",
        "df_obs_participant = df[df.record_id .== participant, [\"fractalselected\", \"outcome\"]]\n",
        "\n",
        "#Also get whole dataset for just one participant\n",
        "df_participant = df[df.record_id .== participant,:]\n",
        "\n",
        "# #select the reward probabilities for simulated_actions\n",
        "df_probs_participant = df[df.record_id .== participant, [\"rewardProbFractal1\", \"rewardProbFractal2\", \"rewardProbFractal3\"]]\n",
        "println(\"Unique record_id values: \", unique_values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Simulate Actions/Inputs based on RewardProbs/Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#### SIMULARTING TO SEE WHAT HAPPENS WITH DIFFERENT PARAMETER SETTINGS ####\n",
        "\n",
        "\n",
        "\n",
        "#Set true probs\n",
        "true_probs = [collect(row) for row in eachrow(df_probs_participant)] #need to convert float to integer\n",
        "\n",
        "\n",
        "#Set initial choice\n",
        "init_choice = rand(Categorical([1/3, 1/3, 1/3]))\n",
        "initial_obs = rand(Bernoulli(true_probs[1][init_choice]))\n",
        "next_input = (init_choice,initial_obs)\n",
        "#Set agent parameters\n",
        "set_parameters!(agent, Dict(\"softmax_precision\" => 5, \n",
        "                            #Default = 4\n",
        "                            # 4 is giving me very noisy responses... 5 works well w/xprob at -1.3\n",
        "                            # \"xvol_volatility\" => -2,\n",
        "                            #default = -2\n",
        "                            \"xprob_volatility\" => -2,\n",
        "                            #default = -2\n",
        "                            #-1.28 seems to be the best number; 1.27-1.28. Cannot be estimated above -1.26, and bad xvol below -1.3\n",
        "                            (\"xvol\", \"volatility\") => -2,1,\n",
        "                            (\"xvol\", \"drift\") => 0,\n",
        "                            (\"xvol\", \"autoconnection_strength\") => 1,\n",
        "                            (\"xvol\", \"initial_mean\") => 0,\n",
        "                            (\"xvol\", \"initial_precision\") => 1,\n",
        "                            \"xbin_xprob_coupling_strength\" => 1,\n",
        "                            \"xprob_initial_mean\" => 0,\n",
        "                            \"xprob_xvol_coupling_strength\" => 1, \n",
        "                            #initial = 1\n",
        "                            #increasing when xprob_vol low (causing low xvol) leads to flat xprob. \n",
        "                            #When xprob_vol high, higher levels flatten both xprob AND xvol. \n",
        "                            #decreasing to 0.2 flattens xvol and sorta xprob\n",
        "                            \"xprob_drift\" => 0,\n",
        "                            # Cannot really change much w/o model freaking out\n",
        "                            \"xprob_autoconnection_strength\" => 0.96, \n",
        "                            # HUGE effects from changes -- best range is 0.95 - 1.01.\n",
        "                            # Seems like 1 = not autoconnection, <1 = increasing regression to 0.5, >1 = increasing regression AWAY from 0.5 (lower if last estimate was <0.5; higher if >0.5)\n",
        "                            # Cannot go far above 1!!!\n",
        "\n",
        "                            ))\n",
        "reset!(agent)\n",
        "\n",
        "#Save the inputs\n",
        "inputs = []\n",
        "simulated_actions = []\n",
        "\n",
        "push!(inputs, next_input)\n",
        "\n",
        "#Simulate\n",
        "for i = 2:length(true_probs) ##start at trial 2 (since trial 1 is random),and go to the length of the total trials\n",
        "\n",
        "\n",
        "    #Update HGF and generate action\n",
        "    action = single_input!(agent, next_input)\n",
        "\n",
        "    #generate new observation based on action\n",
        "    new_observation = rand(Bernoulli(true_probs[i][action]))\n",
        "\n",
        "    #Save the next input as the action and the observation\n",
        "    next_input = (action, new_observation)\n",
        "\n",
        "    push!(simulated_actions, action)\n",
        "    push!(inputs, next_input)\n",
        "end\n",
        "\n",
        "#remove the last input\n",
        "pop!(inputs)\n",
        "\n",
        "## Get the trials on which the bandit was selected for later graphing\n",
        "\n",
        "# Create an empty DataFrame of length inputs that's all missing for the fractal reward or punishment\n",
        "df_actions = DataFrame(\n",
        "    trial = 1:length(inputs), \n",
        "    fractal1HGF = Vector{Union{Missing, Int}}(missing, length(inputs)),\n",
        "    fractal2HGF = Vector{Union{Missing, Int}}(missing, length(inputs)),\n",
        "    fractal3HGF = Vector{Union{Missing, Int}}(missing, length(inputs))\n",
        ")\n",
        "# get_history(agent, (\"action\")) #this gets you a vector of which fractal was picked\n",
        "\n",
        "\n",
        "# Populate the DataFrame based on inputs\n",
        "for (index, (action, result)) in enumerate(inputs)\n",
        "    # Convert true/false to 1/0\n",
        "    result_value = result ? 1 : 0\n",
        "    \n",
        "    # Determine which fractal column to update\n",
        "    if action == 1\n",
        "        df_actions.fractal1HGF[index] = result_value\n",
        "    elseif action == 2\n",
        "        df_actions.fractal2HGF[index] = result_value\n",
        "    elseif action == 3\n",
        "        df_actions.fractal3HGF[index] = result_value\n",
        "    end\n",
        "end\n",
        "\n",
        "# Add colors for the losses vs. rewards\n",
        "colors3 = [ismissing(x) || isnan(x) ? :transparent : (x > 0 ? :green : :red) for x in df_actions.fractal3HGF]\n",
        "colors2 = [ismissing(x) || isnan(x) ? :transparent : (x > 0 ? :green : :red) for x in df_actions.fractal2HGF]\n",
        "colors1 = [ismissing(x) || isnan(x) ? :transparent : (x > 0 ? :green : :red) for x in df_actions.fractal1HGF]\n",
        "\n",
        "\n",
        "#Plot belief trajectories\n",
        "xvol_plot = plot_trajectory(agent,\"xvol\")\n",
        "\n",
        "# Temporarily suppress warnings\n",
        "global_logger(ConsoleLogger(stderr, Logging.Error))\n",
        "\n",
        "\n",
        "xprob3_plot = plot_trajectory(agent, \"xbin3\")\n",
        "plot!(xprob3_plot, df_participant.trial, df_participant.rewardProbFractal3, label=\"Bandit 3 true rew prob\", color=:blue, linewidth=3)\n",
        "scatter!(xprob3_plot, df_actions.trial, df_actions.fractal3HGF, label=\"Bandit 3 observations\", color=colors3)\n",
        "\n",
        "xprob2_plot = plot_trajectory(agent, \"xbin2\")\n",
        "plot!(xprob2_plot, df_participant.trial, df_participant.rewardProbFractal2, label=\"Bandit 2 true rew prob\", color=:blue, linewidth=3)\n",
        "scatter!(xprob2_plot, df_actions.trial, df_actions.fractal2HGF, label=\"Bandit 2 observations\", color=colors2)\n",
        "\n",
        "xprob1_plot = plot_trajectory(agent, \"xbin1\")\n",
        "plot!(xprob1_plot, df_participant.trial, df_participant.rewardProbFractal1, label=\"Bandit 1 true rew prob\", color=:blue, linewidth=3)\n",
        "scatter!(xprob1_plot, df_actions.trial, df_actions.fractal1HGF, label=\"Bandit 1 observations\", color=colors1)\n",
        "\n",
        "\n",
        "# Combine all plots into a single figure with a specified layout\n",
        "combined_plot = plot(xvol_plot, xprob3_plot, xprob2_plot, xprob1_plot, layout = (4,1), size = (1500,800), legend=false)\n",
        "\n",
        "# Display the combined plot\n",
        "display(combined_plot)\n",
        "\n",
        "# savefig(combined_plot, \"./visualizations/HGFtrajectories_id60.png\")  # Saves the plot as a PNG file\n",
        "\n",
        "# Reset logger to default\n",
        "global_logger(ConsoleLogger(stderr, Logging.Info)\n",
        "\n",
        "# #Plot\n",
        "# plot_trajectory(agent, \"xvol\")\n",
        "# plot_trajectory(agent, \"action\")\n",
        "# plot_trajectory(agent, \"xbin1\")\n",
        "# plot_trajectory(agent, \"xbin2\")\n",
        "# plot_trajectory(agent, \"xbin3\")\n",
        "# action_plot = plot_trajectory(agent,\"action\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "colors1\n",
        "# df_actions.fractal1HGF\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Parameter Estimation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Estimate Parameters for all participants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Estimate parameters for one participant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### PARAMETER ESTIMATION FOR SINGLE PARTICIPANT ###\n",
        "\n",
        "#agent should already be defined above\n",
        "\n",
        "#Set prior distributions for each parameter THAT WE WANT TO ESTIMATE!!!!!!!!!!!!!!! \n",
        "#Otherwise, priors fixed to their DEFAULTS in the \"create HGF\"!!\n",
        "#the 1st # = mean, 2nd = SD\n",
        "#lower = lower bound; upper = upper bound -- but will need to include \"truncated\"!!! \n",
        "priors = Dict(\n",
        "    \"softmax_precision\" => truncated(Normal(5, 2),  lower = 0), \n",
        "    (\"xvol\", \"volatility\") => Normal(-2,1),\n",
        "    # (\"xvol\", \"drift\") => 0,\n",
        "    # (\"xvol\", \"autoconnection_strength\") => 1,\n",
        "    (\"xvol\", \"initial_mean\") => Normal(0,1),\n",
        "    (\"xvol\", \"initial_precision\") => 1,\n",
        "\n",
        "    \"xprob_xvol_coupling_strength\" => 1, \n",
        "    # \"xprob_initial_mean\" => Normal(0,1),\n",
        "    \"xprob_volatility\" => Normal(-4, 1),\n",
        "    \"xprob_drift\" => 0,\n",
        "    \"xprob_autoconnection_strength\" => truncated(Normal(1,0) upper = 1), \n",
        "    ##more xprob here...\n",
        "    \"xbin_xprob_coupling_strength\" => 1,\n",
        "\n",
        ")\n",
        "\n",
        "results = fit_model(agent, \n",
        "                    priors, \n",
        "                    inputs, \n",
        "                    simulated_actions;\n",
        "                    sampler = NUTS(),\n",
        "                    n_iterations = 1000,\n",
        "                    n_chains = 2)\n",
        "\n",
        "plot(results)\n",
        "\n",
        "#Plot the posterior distribution against the prior distribution; if your prior is set differently from the true parameter, then you'd hope to see divergence between prior and posterior!\n",
        "plot_parameter_distribution(results, priors)\n",
        "\n",
        "get_posteriors(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9TMXoZuEEib"
      },
      "source": [
        "## Graveyard code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "YT-jayPvEEib",
        "outputId": "3e26ab9c-fdfe-469d-fba8-56d2ece2dfa8"
      },
      "outputs": [],
      "source": [
        "# #but first adjust the reward probabilities to be roughly the min and max for the xprobabilities\n",
        "# df_60_graphing = df_60\n",
        "# cols = names(df_60_graphing)  # Targeting all columns for replacement\n",
        "# for col in cols\n",
        "#     df_60_graphing[!, col] = map(x -> begin\n",
        "#         if ismissing(x)\n",
        "#             x  # Keep missing values as missing\n",
        "#         else\n",
        "#             if x == 0.15 #min prob\n",
        "#                 -4.5\n",
        "#             elseif x == 0.85 #max probability\n",
        "#                 4.5 #max xprob\n",
        "#             elseif x == 1\n",
        "#                 4.3\n",
        "#             elseif x == 0 #if loss, make dot show up at bottom\n",
        "#                 -4.3\n",
        "#             elseif x == 0.5 #50% probabilitiy should be 0 center of graph\n",
        "#                 0.001\n",
        "#             else\n",
        "#                 x  # Return x unchanged if none of the conditions are met\n",
        "#             end\n",
        "#         end\n",
        "#     end, df_60_graphing[!, col])\n",
        "# end\n",
        "\n",
        "# #Add colors array for graphing wins vs. losses\n",
        "# colors3 = [ismissing(x) ? :black : (x > 0 ? :green : :red) for x in df_60_graphing.fractal3HGF]\n",
        "# colors2 = [ismissing(x) ? :black : (x > 0 ? :green : :red) for x in df_60_graphing.fractal2HGF]\n",
        "# colors1 = [ismissing(x) ? :black : (x > 0 ? :green : :red) for x in df_60_graphing.fractal1HGF]\n",
        "\n",
        "# xvol_plot = plot_trajectory(hgf,\"xvol\")\n",
        "\n",
        "# xprob3_plot = plot_trajectory(hgf, \"xbin3\")\n",
        "# plot!(xprob3_plot, df_60_graphing.trial, df_60_graphing.rewardProbFractal3, label=\"Bandit 3 true rew prob\", color=:blue, linewidth=3)\n",
        "# scatter!(xprob3_plot, df_60_graphing.trial, df_60_graphing.fractal3HGF, label=\"Bandit 3 observations\", color=colors3)\n",
        "\n",
        "# xprob2_plot = plot_trajectory(hgf, \"xbin2\")\n",
        "# plot!(xprob2_plot, df_60_graphing.trial, df_60_graphing.rewardProbFractal2, label=\"Bandit 2 true rew prob\", color=:blue, linewidth=3)\n",
        "# scatter!(xprob2_plot, df_60_graphing.trial, df_60_graphing.fractal2HGF, label=\"Bandit 2 observations\", color=colors2)\n",
        "\n",
        "# xprob1_plot = plot_trajectory(hgf, \"xbin1\")\n",
        "# plot!(xprob1_plot, df_60_graphing.trial, df_60_graphing.rewardProbFractal1, label=\"Bandit 1 true rew prob\", color=:blue, linewidth=3)\n",
        "# scatter!(xprob1_plot, df_60_graphing.trial, df_60_graphing.fractal1HGF, label=\"Bandit 1 observations\", color=colors1)\n",
        "\n",
        "\n",
        "# # Combine all plots into a single figure with a specified layout\n",
        "# combined_plot = plot(xvol_plot, xprob3_plot, xprob2_plot, xprob1_plot, layout = (4,1), size = (1500,800), legend=false)\n",
        "\n",
        "# # Display the combined plot\n",
        "# display(combined_plot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # see what premade agents already exist\n",
        "# premade_agent(\"help\")\n",
        "# #load other useful libraries\n",
        "# using HGF\n",
        "# using ActionModels\n",
        "# using Turing\n",
        "# using CSV\n",
        "# using DataFrames\n",
        "# using Plots\n",
        "# using StatsPlots\n",
        "# using LogExpFunctions\n",
        "# using Colors\n",
        "# using StatsModelComparisons"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WD-jEJt5U7ns"
      },
      "source": [
        "# Part 1: Building an analysis pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blAttn8xLv9v"
      },
      "source": [
        "## Overview: \n",
        "\n",
        "In this section, we'll go step-by-step through an analysis pipeline with the HGF.\n",
        "\n",
        "For this example analysis, we will use the task (and data from one subject) from the Iglesias et al. paper described during the presentation. Link to dataset: https://www.research-collection.ethz.ch/handle/20.500.11850/454711.\n",
        "\n",
        "\n",
        "Roughly, this comprises of two steps, that should be iterated:\n",
        "1. Choosing (and potentially implementing) a model.\n",
        "2. Model fitting and checking.\n",
        "\n",
        "We will illustrate these steps below. First, we need to load some example data to use.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bgwj7wfOEEic"
      },
      "outputs": [],
      "source": [
        "# load example data\n",
        "u = Bool[1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0];\n",
        "y = Bool[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0];\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5VgEkG2dnsS"
      },
      "source": [
        "## Step 1: Choose models and priors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxCCoOLyjM5l"
      },
      "source": [
        "### Define first model: biased random\n",
        "\n",
        "When modelling, we always want to control for alternative, simpler explanations. It might be that our subjects are dynamically updating their beliefs in accordance with our assumptions. However, sometimes, they might just be responding rather randomly and not show much learning at all. \n",
        "\n",
        "To control for this possibility, we define a simple alternative model below. This model just takes random actions with a single fixed probability. It does not integrate the data from the task at all. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "Ua0jDXRUjTpR",
        "outputId": "08df7e40-f96b-4e28-f0a1-a558c8f9ccbc"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# Model 1: Simple random agent with bias p \n",
        "#\n",
        "function biased_random(agent::AgentStruct, input)\n",
        "    #Read in parameters\n",
        "    p = agent.params[\"probability\"]\n",
        "    distribution = Distributions.Bernoulli(p)\n",
        "    return distribution\n",
        "end\n",
        "## create model\n",
        "params_m1 = Dict(\"probability\" => .2)\n",
        "m1 = init_agent(biased_random, nothing, params_m1)\n",
        "ysim = give_inputs!(m1, u) ##simulate responses (ysim) for agent m1 (defined above) and inupts u (which we loaded in last cell)\n",
        "\n",
        "# look at some simulated actions\n",
        "scatter(shrink(u, 1.2), legend=:outertop)\n",
        "trajectory_plot!(m1, \"action\", label = \"action\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ai7mT-fZj71X"
      },
      "source": [
        "As we can see from the plot, the actions do not really follow the pattern of the y.\n",
        "\n",
        "Remember: the y are 1 in those trials where the high tone predicts the face vs. house image. People can follow the predictive cue, and their tendency to do so should vary with the actual tendency they have experienced so far. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTXz0RrJkcZp"
      },
      "source": [
        "#### Check recoverability and set priors\n",
        "\n",
        "Below, we fit the data we have simulated above, to check if we get\n",
        "out the true parameter that we used to simulate the data with.\n",
        "\n",
        "If we weren't able to do so, this would compromise the interpretability of our analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        },
        "id": "nsKkgkYRwKQX",
        "outputId": "b0fc7432-62ab-4d17-c114-55f4a4542aa1"
      },
      "outputs": [],
      "source": [
        "# try to recover parameter\n",
        "\n",
        "# provide:\n",
        "# - model : m1\n",
        "# - inputs:  u\n",
        "# - responses:  ysim\n",
        "# - prior distributions for each parameter we want to fit: Beta(1,1)\n",
        "# - the number of iterations\n",
        "# - the number of MCMC chains to run \n",
        "fit = fit_model(m1, u, ysim, Dict(\"probability\" => Beta(1,1)),\n",
        "                n_iterations = 300, n_chains = 2)\n",
        "plot(fit, size=(600,300))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nncQng_jlGeh"
      },
      "source": [
        "The plots below show the results of the model fitting procedure.\n",
        "\n",
        "* On the left: We see the \"traces\" of the MCMC chains. These these are chains of parameter values that were explored while the sampler moved through parameter space (exploring the posterior distribution).\n",
        "  * We see two traces, one for each chain\n",
        "  * We want to see them converge to the same value\n",
        "  * We want the autocorrelation between iterations to be rather low \n",
        "\n",
        "* On the right: The posterior distribution of the \"probability\" parameter.\n",
        "  * The two chains find similiar posteriors, as they should"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkDICpoEl0vT"
      },
      "source": [
        "We can use simulation to investigate the implications of our prior choice on the observed responses.\n",
        "\n",
        "We use the `predictive_simulation_plot` function, providing the prior\n",
        "distribution we want to use for the simulation.\n",
        "\n",
        "The function draws parameter values for each parameter that we specified a prior for and simulates the model forward, producing responses.\n",
        "\n",
        "Different draws lead to different responses (plus the uncertainty in the respones given the same parameters) which gives us a distribution over responses for each trial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "QthFpmXmj-fG",
        "outputId": "db1d271d-9bbc-4525-cc24-95ba5badf4c6"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# Prior predictive sim for model 1\n",
        "#\n",
        "priors_m1 = Dict(\"probability\" => Beta(1,1))\n",
        "predictive_simulation_plot(priors_m1, m1, u, \"action\", \n",
        "                                 n_simulations=100)\n",
        "hline!([.5], label = \"average belief\")\n",
        "xlabel!(\"trials\")\n",
        "ylabel!(\"action (0/1)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43nWrvoxmdyf"
      },
      "source": [
        "### Define second model: RW-KF\n",
        "\n",
        "The second model is the Kalman Filter / Rescorla-Wagner model. This\n",
        "model consists of a `learning_rate` parameter and a `softmax_action_precision` (which controls the variability of responses for a certain belief).\n",
        "\n",
        "The below code implements the belief updates for a single trial. This function will be used to define a new model for use in the `ActionModels` package. \n",
        "\n",
        "`new_value = old_value + learning_rate * (input - transformed_old_value)`\n",
        "\n",
        "The agent has an internal state called `value`, which is the expected value which is updated with the above equation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "IgHrFWSIjbNR",
        "outputId": "26c86741-08da-4b16-b104-9094b5f8675a"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# Model 2: RW model\n",
        "#\n",
        "function binary_rw_softmax(agent::AgentStruct, input)\n",
        "    # Read in parameters and states\n",
        "    learning_rate    = agent.params[\"learning_rate\"]\n",
        "    action_precision = agent.params[\"softmax_action_precision\"]\n",
        "    old_value        = agent.states[\"value\"]\n",
        "    # Sigmoid transform the value (transform into [0,1])\n",
        "    transformed_old_value = 1 / (1 + exp(-old_value))\n",
        "    # Get new value state\n",
        "    new_value = old_value + learning_rate * (input - transformed_old_value)\n",
        "    #Update value\n",
        "    agent.states[\"value\"] = new_value\n",
        "    #Add it to history\n",
        "    push!(agent.history[\"value\"], new_value)\n",
        "    push!(agent.history[\"prediction\"], 1/(1 + exp(-new_value)))\n",
        "    #Pass through softmax to get action probability\n",
        "    action_probability = 1 / (1 + exp(-action_precision * new_value))\n",
        "    #Create Bernoulli normal distribution with mean of the target value and a standard deviation from parameters\n",
        "    distribution = Distributions.Bernoulli(action_probability)\n",
        "    return distribution\n",
        "end\n",
        "## Create agent\n",
        "params_m2 = Dict(\"learning_rate\" => .2, \"softmax_action_precision\" => 0.1)\n",
        "states_m2 = Dict(\"value\" => 0, \"prediction\" => .5)\n",
        "m2 = init_agent(binary_rw_softmax, nothing, params_m2, states_m2)\n",
        "get_params(m2)\n",
        "give_inputs!(m2, u)\n",
        "\n",
        "#\n",
        "# check agent looks sensible:\n",
        "#\n",
        "trajectory_plot(m2, \"prediction\", seriestype=:path)\n",
        "trajectory_plot!(m2, \"action\")\n",
        "scatter!(shrink(u, 1.2), label=\"inputs\")\n",
        "ylabel!(\"probability / outcome\")\n",
        "xlabel!(\"trials\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiIYxeVuoKv-"
      },
      "source": [
        "Above, we used the function `trajectory_plot` to run the model forward for a given sequence of inputs. We can see that the model's predictions adapt to the changing contingencies during the experiment. \n",
        "\n",
        "Try changing the parameter values in `params_m2` to see the effect."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbZTnKHfmut8"
      },
      "source": [
        "#### Check recoverability and set priors\n",
        "\n",
        "Below, we run a little recovery simulation to check if we get\n",
        "out the true parameters that we used to simulate the data with.\n",
        "\n",
        "For this model, as it is quite a bit more complicated than the `biased_random` model (and estimation is thus more difficult), we will simulate 10 datasets that we will fit, in order to \"see through\" the irreducible variability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UVPv7rBbtcK",
        "outputId": "699e9c1c-e831-4ad1-8693-52b6dc2af5ec"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# check we can recover\n",
        "#\n",
        "\n",
        "params_true = Dict(\"learning_rate\" => .2, \"softmax_action_precision\" => 1)\n",
        "ysim = map(1:10) do rep\n",
        "  reset!(m2)\n",
        "  set_params!(m2, params_true)\n",
        "  give_inputs!(m2, u)\n",
        "end\n",
        "\n",
        "priors_m2 = Dict(\"learning_rate\" => Beta(1,1),\n",
        "                 \"softmax_action_precision\" => LogNormal(1,1))\n",
        "\n",
        "fit = [fit_model(m2, u, y, priors_m2,\n",
        "                 n_iterations = 500, n_chains = 2)\n",
        "       for y in ysim]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "5qoLXY5wreWX",
        "outputId": "3f53f546-72fc-460a-b842-f2c1259a0b98"
      },
      "outputs": [],
      "source": [
        "# make plots\n",
        "vnames = [:learning_rate, :softmax_action_precision]\n",
        "\n",
        "plts = map(vnames) do v\n",
        "  # extract the mean of the parameter traces for each chain\n",
        "  if v == vnames[end]\n",
        "    leg = :outerright\n",
        "    else\n",
        "    leg = :none\n",
        "  end\n",
        "  plt = density([mean(c[v]) for c in fit], legend=leg, label=\"density\")\n",
        "  # add individual estimates as scatter\n",
        "  h = ylims(plt)[2]\n",
        "  scatter!([(mean(c[v]), h/4) for c in fit], label=\"posterior means\")\n",
        "  # show the true parameter (defined above)\n",
        "  vline!([params_true[string(v)]], label=\"true value\")\n",
        "  title!(string(v))\n",
        "  plt\n",
        "end\n",
        "plot(plts..., size=(700, 300))\n",
        "ylabel!(\"density\")\n",
        "xlabel!(\"parameter value\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U76bm3jqqZlK"
      },
      "source": [
        "The results of the little recovery simulation are shown above. As you can see, we can estimate the parameters quite well.\n",
        "\n",
        "Again, let's see the effect of different priors on the expected\n",
        "distribution of actions.\n",
        "\n",
        "For this model, we get out the simulated \"belief trajectory\" of the RW models for parameters sampled from the prior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "5zWbmbeHkqxN",
        "outputId": "122e5d6a-af72-4b5d-f3e4-36696b8f96f0"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# Prior predictive sim for model 2\n",
        "#\n",
        "priors_m2 = Dict(\"learning_rate\" => LogNormal(-1,1),\n",
        "                 \"softmax_action_precision\" => LogNormal(0,1));\n",
        "predictive_simulation_plot(priors_m2, m2, u, \"prediction\", n_simulations=100)\n",
        "xlabel!(\"trials\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvZ5usoKtHpT"
      },
      "source": [
        "Shown above is the prior predictive distribution over trajectories, so the distribution of trajectories trajectories simulated with parameters drawn from the prior. The individual samples are shown in grey (heavily overplotted) and their mean is shown in red.\n",
        "\n",
        "The implication of our prior are quite uncertain, as can be seen by the coverage of the space in grey dots.\n",
        "\n",
        "The mean trajectory is shown in red. Try changing the values in the code above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oS8toKmGtrXB"
      },
      "source": [
        "### Define third model: HGF\n",
        "\n",
        "We make use of the `HGF` and `ActionModels` packages to setup our HGF model. \n",
        "\n",
        "We could implement it as above by defining the update functions by hand, however the standard version for models are already implemented for you so we can make use of the `premade_agent` function to use them.\n",
        "\n",
        "For the HGF, there a number of paramters that we need to choose either fixed values or prior distributions for.\n",
        "\n",
        "As a rule, it is always best to start simple with only few parameters given priors and the rest fixed. To decide which values we should use, we can use predictive simulations. Further, we should check the recoverability of each parameter.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgvmbBrqjj7C",
        "outputId": "f7c5f2c6-217f-4d8e-cbbf-d6d5519835e4"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# Model 3: Use standard binary 3-lvl HGF\n",
        "# \n",
        "\n",
        "m3 = premade_agent(\"hgf_unit_square_sigmoid_action\", \n",
        "                   premade_hgf(\"binary_3level\"))\n",
        "\n",
        "# Choose fixed params\n",
        "params_m3 = Dict((\"u\", \"category_means\") => Real[0.0, 1.0],\n",
        "                  (\"u\", \"input_precision\") => Inf,\n",
        "                  (\"x2\", \"initial_mean\") => 0,\n",
        "                  (\"x2\", \"initial_precision\") => 1,\n",
        "                  (\"x3\", \"initial_mean\") => 1,\n",
        "                  (\"x3\", \"initial_precision\") => 1,\n",
        "                  (\"u\", \"x1\", \"value_coupling\") => 1.0,\n",
        "                  (\"x1\", \"x2\", \"value_coupling\") => 1.0,\n",
        "                  (\"x2\", \"x3\", \"volatility_coupling\") => 1.0,\n",
        "                  (\"x2\", \"evolution_rate\") => -6.0,\n",
        "                  (\"x3\", \"evolution_rate\") => -6.0,\n",
        "                  \"sigmoid_action_precision\" => 2)\n",
        "set_params!(m3, params_m3)\n",
        "\n",
        "# define priors \n",
        "priors_m3 = Dict((\"x2\", \"evolution_rate\")   => Normal(-5.0, 2),\n",
        "                 (\"x3\", \"evolution_rate\")   => Normal(-5.0, 2),\n",
        "                  \"sigmoid_action_precision\" => truncated(Normal(1,5), 0, 20));\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0YI_jtwt3Vz"
      },
      "source": [
        "#### Check recoverability and set priors\n",
        "\n",
        "Again, we run a little recovery simulation.\n",
        "\n",
        "This model is again more complicated than the previous, so we will simulate 10 datasets.\n",
        "\n",
        "Below, we set the following priors:\n",
        "* `(\"x2\", \"evolution_rate\")`: this is the $\\omega_2$ parameter from the generative model\n",
        "* `sigmoid_action_precision`: this is the level of \"noise\" in the response model\n",
        "\n",
        "We'll use just 200 iterations, to make things run faster. For an actual analysis you would want to use more than that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxlEQ2HybxyI",
        "outputId": "1bc5828d-f30e-4efb-ac35-58d3e34b8214"
      },
      "outputs": [],
      "source": [
        "# these are the parameters we are generating the responses with\n",
        "true_params = Dict((\"x2\", \"evolution_rate\") => -3, \n",
        "                   \"sigmoid_action_precision\" => 5)\n",
        "\n",
        "# simulate responses\n",
        "ysim = map(1:5) do rep\n",
        "    set_params!(m3, true_params)\n",
        "    y = give_inputs!(m3, u)\n",
        "    reset!(m3)\n",
        "    y\n",
        "end"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAVb3Nxv3P4j",
        "outputId": "f3bfd76b-f25c-4f65-e9f8-aac10f132a59"
      },
      "outputs": [],
      "source": [
        "\n",
        "# fit responses of two subjects only\n",
        "fits = [fit_model(m3, u, y, priors_m3, params_m3, \n",
        "                 n_iterations = 200, n_chains = 1, verbose=false)\n",
        "       for y in ysim[1:2]]\n",
        "fits[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCsk1QO7j698"
      },
      "source": [
        "The standard deviation of the parameters seems low, let's look at the MCMC traces.\n",
        "\n",
        "Another reason to be suspicious is if the fitting procdure takes a long time.\n",
        "This is due to the inference method used (we are using NUTS, the \"No U-Turn Sampler\", a Hamiltonian Monte Carlo algorithm), which moves through parameter space.\n",
        "\n",
        "It might mean that NUTS is hitting its default internal limit of how many steps to take to search for a new sample. If it hitting this limit, that means the\n",
        "model is difficult to fit. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 791
        },
        "id": "7LutZwhFfPbR",
        "outputId": "1fa9642f-e576-4160-d902-e47bc95f596d"
      },
      "outputs": [],
      "source": [
        "plot(fits[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvXmDaC1i7v5"
      },
      "source": [
        "\n",
        "The plots above do not indicate a good fit. The chains have barely moved and the posteriors are anything but conclusive.\n",
        "\n",
        "This might mean that our model is not well informed by the data. Maybe we do not have enough data, or the responses are not constraining the parameters sufficiently.\n",
        "\n",
        "Let's fix more parameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uogvFFki_e5",
        "outputId": "50849cfa-e873-41cc-bbd8-bb0e19d8e462"
      },
      "outputs": [],
      "source": [
        "# we'll fix the evolution rate of the volatility\n",
        "# 1. remove it form the priors (so it won't be estimated)\n",
        "priors_m3 = Dict((\"x2\", \"evolution_rate\")   => Normal(-5.0, 2),\n",
        "                  \"sigmoid_action_precision\" => truncated(Normal(1,5), 0, 20));\n",
        "                  \n",
        "# 2. set a fixed value\n",
        "set_params!(m3, Dict((\"x3\", \"evolution_rate\") => -6.0))\n",
        "\n",
        "# fit again\n",
        "fits = [fit_model(m3, u, y, priors_m3, params_m3, \n",
        "                 n_iterations = 200, n_chains = 2, verbose=false)\n",
        "       for y in ysim[1:5]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "id": "dZWSehIekI4A",
        "outputId": "37787ad0-6e82-46b0-d192-0ca92465ceec"
      },
      "outputs": [],
      "source": [
        "plot(fits[1], size=(500, 500))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbH8z6n-VSCg"
      },
      "source": [
        "\n",
        "This looks better. The chains from different initializations have mixed and the posteriors have a single maximum and are largely in agreement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "9BvIvXM_aBqD",
        "outputId": "c9ebbbf6-8e3d-4c8b-958f-d6d58f48e8a7"
      },
      "outputs": [],
      "source": [
        "\n",
        "# make plots\n",
        "vnames = collect(keys(true_params))\n",
        "plts = map(vnames) do v\n",
        "  if v == vnames[end]\n",
        "    leg = :outerright\n",
        "  else\n",
        "    leg = :none\n",
        "  end\n",
        "  # extract the mean of the parameter traces for each chain\n",
        "  plt = density([mean(c[Symbol(v)]) for c in fits], legend=leg,\n",
        "                label=\"density\")\n",
        "  # add individual estimates as scatter\n",
        "  h = ylims(plt)[2]\n",
        "  scatter!([(mean(c[Symbol(v)]), h/4) for c in fits], label=\"post means\")\n",
        "  # show the true parameter (defined above)\n",
        "  vline!([true_params[v]], label=\"true value\")\n",
        "  title!(string(v))\n",
        "  plt\n",
        "end\n",
        "plot(plts...)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lH6YmbfVxrK6"
      },
      "source": [
        "Above we see the results for the two parameters. There might sometimes be a slight bias in the estimation, but the overall error looks fine. For an actual analysis, this can be acceptable as long as we are able to detect, for example, group differences or correlation with other variables. \n",
        "\n",
        "Go ahead and try the following (in the code above):\n",
        "* change priors:\n",
        "  * try to estimate different parameters\n",
        "* change fixed variables\n",
        "  * change the initial beliefs about volatility or the coupling\n",
        "\n",
        "You can also play around with the `recover` function, defined below. (Note that it may take a long time to run though if you try a complicated model)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMfhpg6xPjL2",
        "outputId": "07b20548-b41b-4277-dbdc-7817fad19a49"
      },
      "outputs": [],
      "source": [
        "function recover(model, priors, theta, u; niter = 10)\n",
        "    set_params!(model, Dict(theta))\n",
        "    reset!(model)\n",
        "    # generate responses \n",
        "    ysim = give_inputs!(model, u)\n",
        "    # now fit \n",
        "    chn = fit_model(model, ysim, u, priors, verbose=false,\n",
        "                    n_iterations=niter, n_chains=2)\n",
        "    return (param = theta[2], chn = chn[Symbol(theta[1])])\n",
        "end\n",
        "\n",
        "val_grid = [-10, -5, -3 ,0]\n",
        "\n",
        "out = [recover(m3, Dict((\"x2\", \"evolution_rate\") => Normal(-5,10)),\n",
        "               (\"x2\", \"evolution_rate\") => val, u, niter = 500)\n",
        "        for val in val_grid, rep in 1:5];"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "osV-aPkbmoEI",
        "outputId": "7f470a52-6380-482c-b4e2-cccf4b88669b"
      },
      "outputs": [],
      "source": [
        "# plot the results\n",
        "scatter(vec([(o.param, mean(o.chn)) for o in out]), label=\"true vs. estimate\")\n",
        "plot!(x -> x, label=\"f(x)=x\")\n",
        "xlabel!(\"true values\")\n",
        "ylabel!(\"estimated values\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUR1YqYkTfjE"
      },
      "source": [
        "What do we see above?\n",
        "\n",
        "1. There is a correlation between the estimates and the true values used in the simulation.\n",
        "2. The estimates are much flatter spanning a smaller space.\n",
        "\n",
        "Regard the second point, this is because we have used informative priors in our Bayesian estimation procedure.\n",
        "\n",
        "The advantage of these is, that they reduce the high noise in the low data regime. However they also lead to bias in the parameter estimates.\n",
        "\n",
        "Still, this bias might be acceptable, and even desirable if the alternative is highly noisy estimates.\n",
        "Note: we might not need to obtain the True parameter estimates. We can also learn from slightly biased estimates, as they might still show relationships with our variables of interest.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaDbGeKldrHD"
      },
      "source": [
        "## Step 2: Model fitting and checking\n",
        "\n",
        "The code below fits the example data `y` with each of the above defined models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lj3XUn9jdtY5",
        "outputId": "7155b484-9a11-46f7-c983-6f8d9af54dea"
      },
      "outputs": [],
      "source": [
        "# Fit each model to the responses y\n",
        "\n",
        "fit1 = fit_model(m1, u, y, priors_m1, params_m1, \n",
        "                  n_iterations = 500, n_chains = 2, verbose=false)\n",
        "\n",
        "fit2 = fit_model(m2, u, y, priors_m2, params_m2, \n",
        "                  n_iterations = 500, n_chains = 2, verbose=false)\n",
        "\n",
        "fit3 = fit_model(m3, u, y, priors_m3, params_m3, \n",
        "                  n_iterations = 500, n_chains = 2, verbose=false)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "id": "w-M21Dim8Xy9",
        "outputId": "0d023255-8675-4132-a5e0-40617a23e12e"
      },
      "outputs": [],
      "source": [
        "chns = [fit1, fit2, fit3]\n",
        "# plot the chains to check for convergence\n",
        "plot([plot(c) for c in chns]...)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIRLGquGIzZj"
      },
      "source": [
        "What we should see is \"good mixing\" (i.e. low autocorrelation of the samples in the chains) and inter-chain agreement on the posteriors.\n",
        "\n",
        "It can happen that the fitting procedure for the HGF gets stuck and fails (when the chain consists of a single value), which can be discovered this way and be resolved by repeating the fitting.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CelptiS61i2x"
      },
      "source": [
        "### Compare models: WAIC\n",
        "\n",
        "Now, let's have a look at the different models.\n",
        "\n",
        "Below we compare the WAIC criterion for each model.\n",
        "This criterion is an estimator for the negative average pointwise predictive log-likelihood. This means, lower values indicate better fits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GzbF-BEe1nOs"
      },
      "outputs": [],
      "source": [
        "# calculate the average predictive performance of the models\n",
        "models = [m1, m2, m3]\n",
        "priors = [priors_m1, priors_m2, priors_m3]\n",
        "params = [params_m1, params_m2, params_m3]\n",
        "chns = [fit1, fit2, fit3]\n",
        "\n",
        "waic = map(zip(1:3, models, priors, params)) do (j, m, prior, fixed)\n",
        "    # calculate the WAIC criterion for model comparison\n",
        "    WAIC(chns[j], m, u, y, prior, fixed)[1].WAIC\n",
        "end"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eyy9kpEO6dA4",
        "outputId": "a557afdd-c3c6-4af4-db84-3d26046e11b7"
      },
      "outputs": [],
      "source": [
        "waic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8YLaL8X1vV3"
      },
      "source": [
        "However, these numbers only provide relative indicators of model fit. Usually you want to also do posterior predictive checks, as described below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vC2E0aN8B46W"
      },
      "source": [
        "### Compare models: Posterior predictive check\n",
        "\n",
        "Now, let's try a different way of comparing the models. A fundamental practice is posterior predictive checking.\n",
        "\n",
        "We can use the provided `predictive_simulation_plot`, providing not a prior distribution (as for a *prior predictive* simulation), but rather the MCMC chain that was returned from the `fit_model` function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "id": "IjLfru6uJUpl",
        "outputId": "5f60c0da-f748-4a19-d731-55a03f0fff0e"
      },
      "outputs": [],
      "source": [
        "p2 = predictive_simulation_plot(fit2, m2, u, \"prediction\", n_simulations=100)\n",
        "# overlay the data\n",
        "scatter!(y, label=\"obs\")\n",
        "p3 = predictive_simulation_plot(fit3, m3, u, (\"x1\", \"prediction_mean\"), \n",
        "                                n_simulations=100)\n",
        "scatter!(y, label=\"obs\")\n",
        "plot(p2, p3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-prWK03oO_t"
      },
      "source": [
        "Shown above is the prior predictive distribution over trajectories, so the distribution of trajectories trajectories simulated with parameters drawn from the prior. The individual samples are shown in grey (heavily overplotted) and their mean is shown in red.\n",
        "\n",
        "Another approach is to try and falsify the models. Thinking about the task, we should be able to come up with some data features that we need to have explained through the modelling. If some models cannot re-create these observed characteristics of the data, we should prefer another model that does.\n",
        "\n",
        "What data feature should be different between the models?\n",
        "1. the `biased_random` will not show learning\n",
        "2. the RW-KF model will not adapt to changing volatility \n",
        "\n",
        "How can we visualize this?\n",
        "\n",
        "1. Look if responses co-vary with inferred time-varying probabilities\n",
        "2. Compare two particular change-points (one when volatility was low and one where it was high) and compare how quickly people adapt to the change. Is it faster for the high-volatility case?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgb81ds4Pq5n"
      },
      "source": [
        "# Part 2: Modelling exercise\n",
        "\n",
        "Here, you will analyse a dataset, which was simulated to mimic a typical dataset\n",
        "of a pharmacological study. A set of n subjects did the same behavioural task twice. Once on placebo and once after taking a dopaminergic drug.\n",
        "\n",
        "Your task is to try to understand what the treatment effect is.\n",
        "\n",
        "## Structure of analysis\n",
        "\n",
        "Go through the below steps during your analysis.\n",
        "\n",
        "1. Choosing or implementing appropriate models for the current behavioural task. \n",
        "  * Visualizing the (observed or simulated) data\n",
        "  * Use existing models or custom built?\n",
        "  * Choose some alternative models for comparison \n",
        "  * Already here compare models with regard to diagnostics (posterior correlations, identifiability)\n",
        "2. Understanding one's assumptions: \n",
        "  * Prior predictive simulation \n",
        "  * Setting priors\n",
        "  * Checking recoverability of parameters\n",
        "3. Analyzing the data\n",
        "  1. Visualising raw data and descriptives\n",
        "  2. Fit all models to all datasets\n",
        "  3. Compare models\n",
        "    * Compare fit: Use goodness-of-fit indices\n",
        "    * Compare with regard to observed statistics (falsifying models)\n",
        "    * Use the same model for all subjects?\n",
        "    * Different models per subject?\n",
        "4. Critique analysis\n",
        "  * Do posterior predictive checks\n",
        "  * Compare model comparison indeces\n",
        "\n",
        "The above steps are just a rough guide and will provide the outline for the following exercises. However, note that the reality of data analysis can be more messy. Sometimes we have to go back and re-check our assumptions, or build a new model after discovering a feature of the observed data our current models could not capture.  \n",
        "\n",
        "The statistician Box, who famously wrote that \"all models are wrong, but some are useful\", described the scientific process as a loop:\n",
        "1. Build\n",
        "2. Fit\n",
        "3. Critique (and go back to 1.)\n",
        "\n",
        "This is important to keep in mind.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pobgMAAA06FB"
      },
      "source": [
        "## Make data\n",
        "\n",
        "The cell below creates the dataset that is to be analyzed.\n",
        "Keep the cell hidden until after you are done with your analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9GYw4KRQM68",
        "outputId": "91e30c53-01d5-4875-95f3-984640c94ed8"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# data generation (should stay hidden until after analysis is done)\n",
        "#\n",
        "\n",
        "function make_agent(k)\n",
        "    if k == 1\n",
        "        p = rand()\n",
        "        p2 = rand()\n",
        "        [init_agent(biased_random, nothing, Dict(\"probability\" => p)),\n",
        "         init_agent(biased_random, nothing, Dict(\"probability\" => p2))]\n",
        "    elseif k == 2\n",
        "        sap = rand() * 10\n",
        "        lr  = rand()\n",
        "        params_rw = Dict(\"softmax_action_precision\" => sap,\n",
        "                        \"learning_rate\" => lr)\n",
        "        params_rw2 = Dict(\"softmax_action_precision\" => sap,\n",
        "                          \"learning_rate\" => sqrt(lr))\n",
        "        [init_agent(binary_rw_softmax, nothing, params_rw, states_rw),\n",
        "         init_agent(binary_rw_softmax, nothing, params_rw2, states_rw)]\n",
        "    elseif k == 3\n",
        "        fixed_params = Dict((\"u\", \"category_means\") => Real[0.0, 1.0],\n",
        "                            (\"u\", \"input_precision\") => Inf,\n",
        "                            (\"x2\", \"initial_mean\") => 0,\n",
        "                            (\"x2\", \"initial_precision\") => 1,\n",
        "                            (\"x3\", \"initial_mean\") => rand([-1,1]),\n",
        "                            (\"x3\", \"initial_precision\") => 1,\n",
        "                            (\"u\", \"x1\", \"value_coupling\") => 1.0,\n",
        "                            (\"x1\", \"x2\", \"value_coupling\") => 1.0,\n",
        "                            (\"x2\", \"x3\", \"volatility_coupling\") => 1.0,\n",
        "                            (\"x2\", \"evolution_rate\") => rand()* -10.0,\n",
        "                            (\"x3\", \"evolution_rate\") => -6.0,\n",
        "                           )\n",
        "        fixed_params2 = Dict((\"u\", \"category_means\") => Real[0.0, 1.0],\n",
        "                            (\"u\", \"input_precision\") => Inf,\n",
        "                            (\"x2\", \"initial_mean\") => 0,\n",
        "                            (\"x2\", \"initial_precision\") => 1,\n",
        "                            (\"x3\", \"initial_mean\") => rand([-1,1]),\n",
        "                            (\"x3\", \"initial_precision\") => 1,\n",
        "                            (\"u\", \"x1\", \"value_coupling\") => 1.0,\n",
        "                            (\"x1\", \"x2\", \"value_coupling\") => 1.0,\n",
        "                            (\"x2\", \"x3\", \"volatility_coupling\") => 1.0,\n",
        "                            (\"x2\", \"evolution_rate\") => fixed_params[(\"x2\", \"evolution_rate\")] + 2,\n",
        "                            (\"x3\", \"evolution_rate\") => -6.0,\n",
        "                           )\n",
        "        [premade_agent(\"hgf_unit_square_sigmoid_action\", \n",
        "                           premade_hgf(\"binary_3level\", fixed_params), \n",
        "                           Dict(\"sigmoid_action_precision\" => rand() * 5)),\n",
        "         premade_agent(\"hgf_unit_square_sigmoid_action\", \n",
        "                       premade_hgf(\"binary_3level\", fixed_params2), \n",
        "                       Dict(\"sigmoid_action_precision\" => rand() * 5))];\n",
        "    else\n",
        "        error(\"not implemented\")\n",
        "    end\n",
        "end\n",
        "\n",
        "function sim_data(u, n_subj=20)\n",
        "    true_k = [rand([1, 2, 3, 3, 3]) for subj in 1:n_subj]\n",
        "    subjs = [make_agent(true_k[subj]) for subj in 1:n_subj]\n",
        "    gen_models = reshape(vcat(subjs...), 2, n_subj)\n",
        "    dataset = [convert(Vector{Bool}, give_inputs!(m, u)) for m in gen_models]\n",
        "    data = map(1:n_subj) do i\n",
        "        uid = prod(string.(rand(1:9, 10)))\n",
        "        cond = rand([[1,2], [2,1]])\n",
        "        order = rand(1:2)\n",
        "        data = Dict()\n",
        "        data[\"uid\"] = uid\n",
        "        if order == 1\n",
        "            data[\"day-1\"] = Dict()\n",
        "            data[\"day-1\"][\"condition\"] = \"placebo\"\n",
        "            data[\"day-1\"][\"u\"] = u\n",
        "            data[\"day-1\"][\"y\"] = dataset[1, i]\n",
        "            data[\"day-2\"] = Dict()\n",
        "            data[\"day-2\"][\"condition\"] = \"drug\"\n",
        "            data[\"day-2\"][\"u\"] = u\n",
        "            data[\"day-2\"][\"y\"] = dataset[2, i]\n",
        "        else\n",
        "            data[\"day-1\"] = Dict()\n",
        "            data[\"day-1\"][\"condition\"] = \"drug\"\n",
        "            data[\"day-1\"][\"u\"] = u\n",
        "            data[\"day-1\"][\"y\"] = dataset[2, i]\n",
        "            data[\"day-2\"] = Dict()\n",
        "            data[\"day-2\"][\"condition\"] = \"placebo\"\n",
        "            data[\"day-2\"][\"u\"] = u\n",
        "            data[\"day-2\"][\"y\"] = dataset[1, i]\n",
        "        end\n",
        "        data\n",
        "    end\n",
        "    data, true_k\n",
        "end\n",
        "\n",
        "using Random\n",
        "Random.seed!(123)\n",
        "dataset, true_k = sim_data(u, 20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1eL5tuGZ8XH",
        "outputId": "8f5d59b1-1de0-4af8-be5d-995412536bb0"
      },
      "outputs": [],
      "source": [
        "\n",
        "#\n",
        "# Include model comparison measures\n",
        "#\n",
        "function WAIC(chn, model, u, y, prior)\n",
        "    m = get_model(model, u, y, prior)\n",
        "    param_mod_predict = m.f(similar(m.args.actions, Missing))\n",
        "    ll = Turing.pointwise_loglikelihoods(m, \n",
        "                                         MCMCChains.get_sections(chn, \n",
        "                                                                 :parameters))\n",
        "    ll = getindex.(Ref(ll), [\"actions[$t]\" for t in 1:length(u)])\n",
        "    ll = permutedims(cat(ll...; dims=3), (2, 1, 3))\n",
        "    waics = [StatsModelComparisons.waic(ll[i,:,:]) for i in 1:size(ll,1)]\n",
        "end\n",
        "\n",
        "\n",
        "function get_model(\n",
        "    agent::AgentStruct,\n",
        "    inputs::Array,\n",
        "    actions::Array,\n",
        "    param_priors::Dict,\n",
        "    fixed_params::Dict = Dict();\n",
        "    impute_missing_actions = false,\n",
        "    sampler = NUTS(),\n",
        "    n_iterations = 1000,\n",
        "    n_chains = 1,\n",
        "    verbose = true,\n",
        ")\n",
        "    #If there are different amounts of inputs and actions\n",
        "    if size(inputs, 1) != size(actions, 1)\n",
        "        throw(\n",
        "            ArgumentError(\n",
        "                \"inputs and actions differs in their first dimension. This is not supported\",\n",
        "            ),\n",
        "        )\n",
        "    end\n",
        "\n",
        "    #Store old parameters \n",
        "    old_params = get_params(agent)\n",
        "\n",
        "    #Unless warnings are hidden\n",
        "    if verbose\n",
        "        #If there are any of the agent's parameters which have not been set in the fixed or sampled parameters\n",
        "        if any(\n",
        "            key -> !(key in keys(param_priors)) && !(key in keys(fixed_params)),\n",
        "            keys(old_params),\n",
        "        )\n",
        "            #Make a warning\n",
        "            @warn \"the agent has parameters which are not specified in the fixed or sampled parameters. The agent's current parameter values are used as fixed parameters\"\n",
        "        end\n",
        "    end\n",
        "\n",
        "\n",
        "    ### Run forward once as testrun ###\n",
        "    #Initialize dictionary for populating with median parameter values\n",
        "    sampled_params = Dict()\n",
        "    #Go through each of the agent's parameters\n",
        "    for (param_key, param_prior) in param_priors\n",
        "        #Add the median value to the tuple\n",
        "        sampled_params[param_key] = median(param_prior)\n",
        "    end\n",
        "    #Set parameters in agent\n",
        "    set_params!(agent, sampled_params)\n",
        "    #Set fixed parameters\n",
        "    set_params!(agent, fixed_params)\n",
        "    #Reset the agent\n",
        "    reset!(agent)\n",
        "    #Run it forwards\n",
        "    test_actions = give_inputs!(agent, inputs)\n",
        "\n",
        "    #If the model returns a different amount of actions from what was inputted\n",
        "    if size(test_actions) != size(actions)\n",
        "        throw(\n",
        "            ArgumentError(\n",
        "                \"The passed actions is a different shape from what the model returns\",\n",
        "            ),\n",
        "        )\n",
        "    end\n",
        "\n",
        "\n",
        "    ### Fit model ###\n",
        "    #Initialize dictionary for storing sampled parameters\n",
        "    fitted_params = Dict()\n",
        "\n",
        "    #Create turing model macro for parameter estimation\n",
        "    @model function fit_agent(actions)\n",
        "\n",
        "        #Give Turing prior distributions for each fitted parameter\n",
        "        for (param_key, param_prior) in param_priors\n",
        "            fitted_params[param_key] ~ param_prior\n",
        "        end\n",
        "\n",
        "        #Set agent parameters to the sampled values\n",
        "        set_params!(agent, fitted_params)\n",
        "        reset!(agent)\n",
        "\n",
        "        #If the input is a single vector\n",
        "        if inputs isa Vector\n",
        "            #Prepare to through one value at a time\n",
        "            iterator = enumerate(inputs)\n",
        "        else\n",
        "            #For an array, go through each row\n",
        "            iterator = enumerate(eachrow(inputs))\n",
        "        end\n",
        "\n",
        "        #For each timestep and input\n",
        "        for (timestep, input) in iterator\n",
        "            #If no errors occur\n",
        "            try\n",
        "\n",
        "                #Get the action probability distribution from the action model\n",
        "                action_probability_distribution = agent.action_model(agent, input)\n",
        "\n",
        "                #If only a single action is made at each timestep\n",
        "                if actions isa Vector\n",
        "\n",
        "                    #If the action isn't missing, or if missing actions are to be imputed\n",
        "                    if !ismissing(actions[timestep]) || impute_missing_actions\n",
        "                        #Pass it to Turing\n",
        "                        actions[timestep] ~ action_probability_distribution\n",
        "                    end\n",
        "\n",
        "                    #If multiple actions are made at each timestep\n",
        "                elseif actions isa Array\n",
        "\n",
        "                    #Go throgh each action distribution\n",
        "                    for (action_indx, distribution) in\n",
        "                        enumerate(action_probability_distribution)\n",
        "\n",
        "                        #If the action isn't missing, or if missing actions are to be imputed\n",
        "                        if !ismissing(actions[timestep, action_indx]) || impute_missing_actions\n",
        "                            #Pass it to Turing\n",
        "                            actions[timestep, action_indx] ~ distribution\n",
        "                        end\n",
        "                    end\n",
        "                end\n",
        "            catch e\n",
        "                #If the custom errortype ParamError occurs\n",
        "                if e isa ParamError\n",
        "                    #Make Turing reject the sample\n",
        "                    Turing.@addlogprob!(-Inf)\n",
        "                else\n",
        "                    #Otherwise, just throw the error\n",
        "                    throw(e)\n",
        "                end\n",
        "            end\n",
        "        end\n",
        "    end\n",
        "\n",
        "    return fit_agent(actions)\n",
        "end\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjAET6aY09qX"
      },
      "source": [
        "## Analysis\n",
        "\n",
        "The structure of the dataset:\n",
        "* A Dictionary with one entry per participant\n",
        "* Each participant has done the task twice, so there is two datasets\n",
        "* The preprocessing code below transforms the data into an array `u` holding the inputs (which were kept the same for every subject and condition) and an array of arrays `y` that contains the responses for each subject (where the rows correspond to subjects and the columns to the two conditions).\n",
        "\n",
        "The data in the first column are the responses in the placebo condition and the data in the second column are the responses in the drug condition.\n",
        "\n",
        "\n",
        "Do the following steps:\n",
        "1. define at least three different models.\n",
        "2. choose priors for some and fixed settings for all other parameters for each model. Remember to use simulations to guide your choices.\n",
        "3. fit different models until you have a good description of the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVY7JD7Ga3hS",
        "outputId": "6928730e-89ae-4703-bf72-ab823d936531"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# First do some preprocessing\n",
        "#\n",
        "\n",
        "# we know that u is always the same\n",
        "u = dataset[1][\"day-1\"][\"u\"]\n",
        "n_subj = length(dataset)\n",
        "# loop over subjects and re-order\n",
        "y = map(1:n_subj) do i\n",
        "    cond = dataset[i][\"day-1\"][\"condition\"]\n",
        "    if cond == \"drug\"\n",
        "        [dataset[i][\"day-2\"][\"y\"], dataset[i][\"day-1\"][\"y\"]]\n",
        "    else\n",
        "        [dataset[i][\"day-1\"][\"y\"], dataset[i][\"day-2\"][\"y\"]]\n",
        "    end\n",
        "end\n",
        "# concatenate responses\n",
        "y = hcat(y...)\n",
        "# have rows correspond to subjects \n",
        "y = permutedims(y, [2, 1])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cq-6BwoRas4A"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# Start by visualizing the data of a number of subjects\n",
        "#\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itaj5PD6xz80",
        "outputId": "d6d4d410-eeb8-497b-b324-173ab00347e2"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# Step 1: select models\n",
        "#\n",
        "\n",
        "# As a starting point, you can use the models from above:\n",
        "m1 = init_agent(biased_random, nothing, Dict(\"probability\" => .5))\n",
        "m2 = init_agent(binary_rw_softmax, nothing, \n",
        "                Dict(\"softmax_action_precision\" => 1, \"learning_rate\" => 1.5), \n",
        "                states_rw)\n",
        "params_m3 = Dict((\"u\", \"category_means\") => Real[0.0, 1.0],\n",
        "                (\"u\", \"input_precision\") => Inf,\n",
        "                (\"x2\", \"initial_mean\") => 0,\n",
        "                (\"x2\", \"initial_precision\") => 1,\n",
        "                (\"x3\", \"initial_mean\") => 1,\n",
        "                (\"x3\", \"initial_precision\") => 1,\n",
        "                (\"u\", \"x1\", \"value_coupling\") => 1.0,\n",
        "                (\"x1\", \"x2\", \"value_coupling\") => 1.0,\n",
        "                (\"x2\", \"x3\", \"volatility_coupling\") => 1.0,\n",
        "                (\"x2\", \"evolution_rate\") => -5.0,\n",
        "                (\"x3\", \"evolution_rate\") => -6.0,\n",
        "                \"sigmoid_action_precision\" => 1.0)\n",
        "m3 = premade_agent(\"hgf_unit_square_sigmoid_action\", \n",
        "                  premade_hgf(\"binary_3level\"))\n",
        "set_params!(m3, params_m3)                  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGiQQtRX1jsr",
        "outputId": "727bd1f4-9fb7-4208-bcba-b78ee1b1669b"
      },
      "outputs": [],
      "source": [
        "# try some priors\n",
        "priors_m1 = Dict(\"probability\" => Beta(1,1))\n",
        "priors_m2 = Dict(\"learning_rate\" => Beta(1,1), \"softmax_action_precision\" => LogNormal())\n",
        "priors_m3 = Dict((\"x2\", \"evolution_rate\") => Normal(-3.0, 2),\n",
        "                 \"sigmoid_action_precision\" => truncated(Normal(3,5), 0, 50));\n",
        "priors = [priors_m1, priors_m2, priors_m3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Mtm6-BezAL0K",
        "outputId": "cdcd7ad4-1786-4d6e-b14d-7518d31f5e10"
      },
      "outputs": [],
      "source": [
        "fixed_params_m1 = Dict()\n",
        "fixed_params_m2 = Dict()\n",
        "fixed_params_m3 = params_m3\n",
        "fixed_params = [fixed_params_m1, fixed_params_m2, fixed_params_m3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4Cfhva-QV7r",
        "outputId": "a266666a-9695-4de4-a615-54bcec84b7cd"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# Step 2: fit models\n",
        "#\n",
        "iters = Iterators.product(1:n_subj, 1:2, 1:3)\n",
        "allfits = map(iters) do (subj, cond, j)\n",
        "    # fit model\n",
        "    chn = fit_model(models[j], u, y[subj, cond], priors[j], fixed_params[j],\n",
        "                    n_iterations=500, n_chains=2, verbose=false)\n",
        "    # calculate the WAIC criterion for model comparison\n",
        "    waic = WAIC(chn[1], models[j], u, y[subj, cond], priors[j], fixed_params[j])[1].WAIC\n",
        "    (chn = chn, WAIC = waic)\n",
        "end\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6AK6WJh9aW42"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# Check the models\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2ojJLNyYfKz"
      },
      "source": [
        "After we have established that the HGF is an appropriate model, we can\n",
        "compare the parameter estimates across groups."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_VByfPNC6_j"
      },
      "outputs": [],
      "source": [
        "# variable to compare\n",
        "v = Symbol(\"(\\\"x2\\\", \\\"evolution_rate\\\")\")\n",
        "# choose colors for the two conditions\n",
        "cols = [RGBA(.1, .1, .9, .1) RGBA(.1, .9, .1, .1)]\n",
        "\n",
        "#\n",
        "# make plot\n",
        "#\n",
        "p1 = violin([1 2], [mean(c.chn[v]) for c in allfits[:,:,3]], legend=:none, color=cols)\n",
        "boxplot!([1 2], [mean(c.chn[v]) for c in allfits[:,:,3]], legend=:none, color=cols)\n",
        "scatter!(repeat([1 2], n_subj, 1), [mean(c.chn[v]) for c in allfits[:,:,3]], legend=:none, color=cols)\n",
        "\n",
        "#\n",
        "# if we exclude the participants that are not well explained by \n",
        "# the model\n",
        "#\n",
        "# for this: you need to define the vector ids_m3, holding those subjects\n",
        "# that are best described by the HGF model:\n",
        "ids_m3 = findall(est_k .== 3)\n",
        "chns = allfits[ids_m3, :, 3]\n",
        "p2 = violin([1 2], [mean(c.chn[v]) for c in chns], legend=:none, color=cols)\n",
        "boxplot!([1 2], [mean(c.chn[v]) for c in chns], legend=:none, color=cols)\n",
        "scatter!(repeat([1 2], n_subj, 1), [mean(c.chn[v]) for c in chns], legend=:none, color=cols)\n",
        "\n",
        "plot(p1, p2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgNZEAVdTa01"
      },
      "source": [
        "# Setting Priors\n",
        "\n",
        "A common question is: How do I decide on the prior settings?\n",
        "\n",
        "And even more importantly, which variables are we even estimating and which are assumed fixed?\n",
        "\n",
        "Here we'll treat this. Below you find some basic considerations and a guide for the parameters of the binary 3-level HGF we are using in this tutorial.\n",
        "\n",
        "\n",
        "## Some basic considerations for setting priors\n",
        "\n",
        "1. What space does the parameter \"live in\"?\n",
        "  * for a probability, we should only allow for values between 0 and 1.\n",
        "  * for discrete variables, we need to choose a distribution on discrete numbers (categorical, poisson, binomial...)\n",
        "  * etc.\n",
        "\n",
        "2. Do you have prior knowledge?\n",
        "  * i.e. you might want to use estimates from prior work\n",
        "  * you know that one condition should give higher estimates\n",
        "    * prior predictive simulations can help set priors that lead to observed behavior in line with your prior information\n",
        "\n",
        "3. Further considerations:\n",
        "  * Probability theory says: If you have enough data, the influence of the prior will become negligible. In practice, we never have enough data though... \n",
        "  * You can start with uniform priors (giving ML estimates)\n",
        "  * Regularization: do you have problems fitting the model with\n",
        "  non-informative priors?\n",
        "    * constrain certain parameters more in order to improve the model fitting \n",
        "  * How do you want to explain the behavior?\n",
        "    * if there is a coupling parameter between beliefs and actions, and we are interested in explaining beliefs, we would want to bias the coupling to be strong (in thus explain behavior in terms of belief trajectories)\n",
        "      * this can be controversial and has to be well argued\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaUlfSmB7k-7"
      },
      "source": [
        "### Parameters\n",
        "\n",
        "* $\\mu_k^{(0)}$ (initial belief mean): What do subjects know?\n",
        " * For the second level (tendency), we may be happy with a value of 0 as this means an first-level expectation that gives equal probability of 0.5 to each of the two possible observations (0 and 1).\n",
        " * For the third level (volatility), $\\mu_3^{(0)}$, it is a bit harder and depends on the instructions. Do people know the probability of reversals (from this we might derive appropriate beliefs). There is other considerations, such as if this might represent a variable that differs between individuals (in which case we might want to estimate it).\n",
        "\n",
        "* $\\sigma_k^{(0)}$ (intial precision at level k): this can be used to fix variables. This is because for a prior variance of 0, no data can update the belief. \n",
        " * Otherwise we might want to regularize the belief or less.\n",
        "\n",
        "* $\\kappa$ (coupling): As the aim of the modelling is often to explain the actions as result of differences in belief updating, the coupling is often fixed to 1. A value of 0 would sever two levels and reduces the three level HGF to a two-level, constant volatility model.\n",
        "\n",
        "* $\\omega$ (evolution rate): This will often be the target of fitting. The prior should allow for a wide range of values, but constraining this parameter often improves model fitting. Through simulation, you will often realize that negative values lead to sensible results, but keep in mind that the effective learning rate depends on a dynamic interplay of the different levels, and you should perform simulations where you vary multiple parameters.\n",
        "\n",
        "Some models also have:\n",
        "* $\\alpha$ (perceptual uncertainty): This is the width of the category priors. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJHS-GvDZb3a"
      },
      "source": [
        "# Further exercises\n",
        "\n",
        "### Chaning the generative model\n",
        "\n",
        "* What changes can you come up with?\n",
        "* How would this change alter the inference process of the participants? Would it be harder or easier? Why?\n",
        "* How would you need to change the model?\n",
        "* What difficulties might be related (aka why NOT do it?) \n",
        "\n",
        "\n",
        "### Volatility\n",
        "\n",
        "Create two designs:\n",
        "  * Constant volatility\n",
        "  * Time-varying volatility\n",
        "  * Which models would be most appropriate for either?\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "pobgMAAA06FB"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Julia 1.10.2",
      "language": "julia",
      "name": "julia-1.10"
    },
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia",
      "version": "1.10.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
